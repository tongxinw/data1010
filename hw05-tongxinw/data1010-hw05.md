---
jupyter:
  jupytext:
    formats: ipynb,md
    text_representation:
      extension: .md
      format_name: markdown
      format_version: '1.1'
      jupytext_version: 1.2.4
  kernelspec:
    display_name: Julia 1.2.0
    language: julia
    name: julia-1.2
---

<style>
@media print
{
h2 {page-break-before:always}
}
</style>

# Homework 05

### Brown University  
### DATA 1010  
### Fall 2019


## Problem 1

Label each of the following four estimators as either (i) biased and
  consistent, (ii) biased and inconsistent, (iii) unbiased and
  consistent, or (iv) unbiased and inconsistent.  The matching will be
  one-to-one.

(a) $X_1, X_2, \ldots$ are i.i.d. Bernoulli random variables with
    unknown $p$ and estimator 
    $$\widehat{p} = \frac{1}{n}\sum^n_{i=1}X_i$$

(b) $X_1, X_2, \ldots$ are i.i.d. $\mathcal{N}(\mu,\sigma^2)$, with
    unknown $\mu$ and $\sigma^2$ and estimator 
    $$\widehat{\sigma}^2 = \frac{\displaystyle{\sum^n_{i=1}(X_i-\bar{X})^2}}{n}$$

(c) $X_1, X_2, \ldots$ are i.i.d. uniform random variables on an
    unknown bounded interval. For $n\geq 100$ we estimate the
    mean using
    $$\widehat{\mu} = \frac{\displaystyle{\sum^{100}_{i=1}X_i}}{100}$$
    
(d) $X_1, X_2, \ldots$ are i.i.d. $\mathcal{N}(\mu,\sigma^2)$, with
    unknown $\mu$ and $\sigma^2$. For $n\geq 100$ we estimate the
    standard deviation using
    $$\widehat{\sigma} = \sqrt{\frac{\displaystyle{\sum^{100}_{i=1}(X_i-\overline{X})^2}}{99}}$$


(a)  

$\widehat{p}$ is unbiased and consistant. The law of large numbers implies that any unbiased estimator which is an average of $n$ i.i.d. random variables is consistent.   

(b)  

$\widehat{\sigma}^2$ is biased but consistant.      

(c)  

$\widehat{\mu}$ is unbiased and inconsistant.   

(d)  

$\widehat{\sigma}$ is biased and inconsistant.


## Problem 2 

Suppose that $X_1, \dots, X_n$ are independent
  $\mathrm{Unif}[0, \theta]$ random variables, where $\theta$ is an
  unknown parameter, and consider the
  following estimators for $\theta$: 
  $$\widehat{\theta}_1 = \max(X_1, \dots, X_n),  \qquad \widehat{\theta}_2 = 2 \cdot \frac{X_1 + \cdots + X_n}{n}$$
    
Hint: this problem is pretty calculus intensive. SymPy is your friend.


(a) Find the CDF of $\widehat{\theta}_1$.


$P(\widehat\theta_{1})<x = P(\max(X_1, \dots, X_n)<x)= P(X_{1}<x, X_{2}<x, \dots, X_{n}<x) = (\frac{x}{\theta})^{n}$    


(b) Recall that if $F_{\widehat{\theta}_1}(x)$ and
    $f_{\widehat{\theta}_1}(x)$ are the CDF and PDF of
    $\widehat{\theta}_1$ respectively,
    then $
    \displaystyle{\frac{d}{dx}F_{\widehat{\theta}_1}(x) =
      f_{\widehat{\theta}_1}(x)}$.
    Differentiate your answer to (a) to find the PDF of $\widehat{\theta}_1$. 


$f_{\widehat\theta_{1}}(x) = \frac{nx^{n-1}}{\theta^{n}}$    

```julia
using SymPy
@vars x n θ
F_1(x) = (x/θ)^n
f_1(x) = diff(F_1(x), x)
f_1(x)
```

(c) Show that $\widehat{\theta}_1$ is consistent. 


As $n\rightarrow\infty$, $\widehat\theta_{1} = \max(X_1, \dots, X_n) = \theta$ as the maximum is get extremely close to $\theta$, thus $\widehat\theta_{1}$ is consistant      


(d) Find $\mathbb{E}\left[{\widehat{\theta}_1}\right]$ and
    $\mathbb{E}\left[\widehat{\theta}_2\right].$ Which estimator is biased? 


$\mathbb{E}[\widehat\theta_{1}] = \frac{n\theta}{n+1} \ne \theta$, thus $\widehat\theta_{1}$ is biased.      
$\mathbb{E}[\widehat\theta_{2}] = \mathbb{E}(2\frac{X_{1} + X_{2} + \dots + X_{n}}{n}) = 2\mathbb{E}\frac{(X_{1} + X_{2} + \dots + X_{n})}{n} = 2\frac{\theta}{2} = \theta$, thus $\widehat\theta_{2}$ is unbiased.

```julia
integrate(x*f_1(x), (x,0,θ))
```

(e) Find $\operatorname{Var}\left({\widehat{\theta}_1}\right)$ and
    $\operatorname{Var}\left(\widehat{\theta}_2\right).$ Which estimator has lower
    variance? 



$Var(\widehat\theta_{1}) = \mathbb{E}(\widehat\theta_{1}^2) - (\mathbb{E}(\widehat\theta_{1}))^2 = \frac{n\theta^2}{(n+2)(n+1)^2}$ and $Var(\widehat\theta_{2}) = \frac{4}{n^2}Var(X_{1} + X_{2} + \dots + X_{n}) = \frac{4}{n^2}(Var{(X_1)} + \dots + Var(X_n)) = \frac{\theta^2}{3n}$      
Thus, $\widehat\theta_{1}$ has lower variance

```julia
integrate(x^2*f_1(x), (x,0,θ))
simplify((n*θ^2/(n+2))-((n/(n+1))θ)^2)
```

(f) Show that the mean squared error of $\widehat{\theta}_1$
    is less than the mean squared error of $\widehat{\theta}_2$
    whenever $n \geq 3$. 


$MSE(\widehat\theta_{1}) = Var(\widehat\theta_{1}) - (Bias(\widehat\theta_{1}))^2 = \frac{2\theta^2}{(n+1)(n+2)}$      
$MSE(\widehat\theta_{2}) = Var(\widehat\theta_{2}) - (Bias(\widehat\theta_{2}))^2 = \frac{\theta^2}{3n}$      
$\Delta = MSE(\widehat\theta_{1}) - MSE(\widehat\theta_{2}) = \frac{-n(n-3)-2}{3n(n^2+3n+2)}\theta^2$, when $n \geq 3$, we get $\Delta$ is alwasy negative, thus $MSE(\widehat\theta_{1})$ is less than $MSE(\widehat\theta_{2})$ whenever $n \geq 3$.

```julia
simplify((n*θ^2/(n+2))-((n/(n+1))θ)^2 + θ^2/(n+1)^2)
```

```julia
simplify(2/(n^2+3n+2)-1/3n)
```

## Problem 3

(a) **Hoeffding's inequality** says that if
    $Y_1, Y_2, \ldots$ are independent random variables with the
    property that $\mathbb{E}[Y_i] = 0$ and $a_i \leq Y_i \leq b_i$ for all
    $i$, then for all $\epsilon>0$ and $t > 0$, we have
    $$\mathbb{P}\left(Y_1 + Y_2 + \cdots + Y_n \geq \epsilon \right) \leq
      e^{-t \epsilon} \prod_{i=1}^n e^{t^2(b_i-a_i)^2/8}.$$
    
   Use Hoeffding's inequality to show that if $X_1, X_2, X_3, \ldots$
    is a sequence of independent $\operatorname{Bernoulli}(p)$ random
    variables, then for all $\alpha > 0$, the interval
    $\left(\overline{X}_n - \sqrt{\frac{1}{2n}\log(2/\alpha)},
      \overline{X}_n + \sqrt{\frac{1}{2n}\log(2/\alpha)}\right)$ is a
    confidence interval for $p$ with confidence level $1 -
    \alpha$. Explain what happens to the width of this confidence
    interval if $n$ gets large, and also what happens to the width
    if $\alpha$ is made very small.
    
(b) As above, consider $n$ independent
    $\operatorname{Bernoulli}(p)$'s. Find the normal-approximation
    confidence interval for $p$

(c) As above, consider $n$ independent
    $\operatorname{Bernoulli}(p)$'s. Find the Chebyshev confidence
    interval for $p$. (Chebyshev's inequality says that the probability of any random variable deviating from its mean by more than $k$ standard deviations is no more than $1/k^2$.)

(d) Find the numerical values of the half-widths for each of the
    above confidence intervals when $p = \frac{1}{2}$, $n = 1000$, and
    $\alpha = 0.05$ (approximating $\overline{X}$ as $p$). 


(a)   
Let $Y_{i} = \frac{X_{i}-p}{n}$.   
Since $X$ is Bernoulli random variables, then $x = 0$ or $1$, thus $a_{i} = -\frac{p}{n}, b_{i} = \frac{1-p}{n}$ and $b_{i} - a_{i} = \frac{1}{n}$.   
$P(\bar{x} - p \geq \epsilon) \leq e^{(\frac{t^2}{8n} - t\epsilon)}$  
Let $t = 4n\epsilon$ in order to minimize the right hand side, we get $P(\bar{x} - p \geq \epsilon) \leq e^{-2n\epsilon^2}$.   
Since $\mathbb{E}(Y) = 0$, thus symmetric, thus $P(|\bar{x} - p| \geq \epsilon) \leq 2e^{-2n\epsilon^2}$.   
Let $\epsilon = \sqrt{\frac{1}{2n}\log(2/\alpha)}$, then we get $\alpha = 2e^{-2n\epsilon^2}$, and $P(|\bar{x} - p| \geq \sqrt{\frac{1}{2n}\log(2/\alpha})) \leq \alpha$, which is the interval $\left(\overline{X}_n - \sqrt{\frac{1}{2n}\log(2/\alpha)},\overline{X}_n + \sqrt{\frac{1}{2n}\log(2/\alpha)}\right)$ is a confidence interval for $p$ with confidence level $1 - \alpha$.  
If n get large, then the confidence interval will become smaller since $\sqrt{\frac{1}{2n}\log(2/\alpha)}$ -> 0.   
If $\alpha$ is made very small, then the interval will become very large since the confidence level $1-\alpha$ is high, then the interval we need is getting larger.



(b)  
The confidence interval for p is $\bar{X} \pm z_{1-\alpha/2} \sqrt{\frac{\bar{X}(1-\bar{X})}{n}}$


(c)   

The confidence interval for p is $p(|\bar{X} - p| > k\sigma) \leq \frac{1}{k^2}$   
Let $\frac{1}{k^2} = \alpha$, then $k = \sqrt{\frac{1}{\alpha}}$,   
the confidence interval is $\bar{X} \pm \sqrt{\frac{\bar{X}(1-\bar{X})}{\alpha n}}$

```julia
# (d)
p, n, α = 1/2, 1000, 0.05
# Hoeffding's inequality
println(sqrt(1/2n * log(2/α)))
# normal-approximation
println(1.96*sqrt(1/(4n)))
# Chebyshev confidence
println(sqrt(1/(4*n*α)))
```

## Problem 4

I drew 6 observations from an undisclosed distribution and obtained the following results:
  `u = [6.19,7.048,6.143,5.459,4.603,4.335]`
  
I also drew 8 observations from another undisclosed distribution and got 
  `v = [8.924,4.698,6.095,4.223,3.643,1.624,1.444,6.309]`
  
(a) Determine whether the Wald hypothesis test (with significance $\alpha = 0.05$) rejects the null hypothesis that the mean of the two distributions are equal. 

(b) Repeat with Welch's t-test in place of the the Wald test. 


**Solution:**     
(a) The value of $\bar u - \bar v$ devided by its estimated standard error is $1.03$, which is smaller than $1.96$, thus the Wald hypothesis test fail to reject the null hypothesis.

```julia
using Statistics, Distributions
u = [6.19,7.048,6.143,5.459,4.603,4.335]
v = [8.924,4.698,6.095,4.223,3.643,1.624,1.444,6.309]
m_u, m_v = mean(u), mean(v)
s_u, s_v = std(u), std(v)
n_u, n_v = length(u), length(v)
var_ = sqrt(s_u^2/n_u + s_v^2/n_v)
z = (m_u - m_v)/var_
# ccdf(Normal(0, 1), z)
```

(b) $0.16 > 0.05$, thus we fail to reject the null hypothesis.

```julia
a = s_u^2/n_u
b = s_v^2/n_v
ν = (a + b)^2 / (a^2/(n_u-1) + b^2/(n_v-1))
t = (m_u - m_v)/sqrt(a+b)
ccdf(TDist(ν), t)
```

<!-- #region -->
## Problem 5

Consider a distribution $\nu$ which is known only via a dozen samples therefrom, the values of which are
```julia
    [8.924,4.698,6.095,4.223,3.643,1.624,1.444,6.309]
```

(a) Obtain a bootstrap estimate of the standard deviation of the
    median of five independent samples from $\nu$.

(b) The actual standard deviation of the median of 5 samples from
    $\nu$ is approximately 2.14. How close is the value you found?
    Could you have gotten as close as desired to this value by
    choosing sufficiently many bootstrap re-samplings?
<!-- #endregion -->

```julia
#(a)
data = [8.924,4.698,6.095,4.223,3.643,1.624,1.444,6.309]
boot_estimates = [median(sample(data,5)) for _ in 1:10^5]
std(boot_estimates)
```

(b)    
The result we get above is kind far from the actual standard deviation. The percentage error is 32.7%.  
We cannot, sufficient re-samplings can only tell us the standard deviation of the 8 samples, not the actually distribution, as the 8 samples does not necessarily represent the whole sample well. 

```julia
string(abs(1.44 - 2.14)/2.14 * 100) * '%'
```

## Problem 6

Consider a population of patients who have had a recent heart attack. A randomized trial is conducted in which each patient is assigned with equal probability (and independently of any attribute of the patient) to either a heart medication regimen or a placebo. Each patient has a unknown genetic attribute $X$ which is uniformly distributed on $[0,1]$.

Suppose that the probability that a patient will comply with the regimen (that is, take the medication as prescribed) is $(1+X)/2$. The conditional probability that they will survive the following decade, given $X$ and given that they take the drug, is $3X/4$. The conditional probability that they will survive the following decade is, given $X$ and given that they do not take the drug, is $(X+1)/4$.

(a) Fill out the following tables, indicating the probability of each outcome. The eight numbers should sum to 1. 

<table>
  <tr>
    <th>Drug condition</th>
    <td>survives</td>
    <td>does not survive</td>
  </tr>
  <tr>
    <td>complaint</td>
    <td>5/32</td>
    <td>7/32</td>
  </tr>
  <tr>
    <td>non-compliant</td>
    <td>1/24</td>
    <td>1/12</td>
  </tr>
</table>
<table>
  <tr>
    <th>Placebo condition</th>
    <td>survives</td>
    <td>does not survive</td>
  </tr>
  <tr>
    <td>complaint</td>
    <td>7/48</td>
    <td>11/48</td>
  </tr>
  <tr>
    <td>non-compliant</td>
    <td>1/24</td>
    <td>1/12</td>
  </tr>
</table>

Hint: you want to work out the conditional probabilities of each event given $X$, and then find the expected value of the resulting conditional probability by integrating against the density of $X$. 

(b) Does a randomly selected patient have a higher conditional probability of surviving if they take the drug or if they do not? Does comparing the survival probabilities for the drug and placebo conditions give the correct answer to this question?

(c) Suppose you know yourself well enough to be confident that you'd be relatively unlikely to comply with a prescription regimen if you had participated in this clinical trial. Based on the given probability model, should you take the drug? Would you get the right answer or the wrong answer if you just compared the survival rates for compliant and noncompliant patients?

```julia
# (a)
println(integrate((3x/4)*(1+x)/4, (x,0,1))) # survives & compliant
print(integrate((1+x)*(4-3x)/16, (x,0,1)), '\n') # die & compliant
print(integrate((1-x^2)/16,(x,0,1)), '\n') # survives & not compliant
print(integrate(((1-x)*(3-x))/16,(x,0,1)),'\n') # die & not compliant
print(integrate((1+x)^2/16,(x,0,1)),'\n') # survives & compliant & placebo
print(integrate(1/2 * (1+x)/2 * (1 - (x + 1)/4),(x,0,1)),'\n') # die & compliant & placebo
print(integrate((1-x^2)/16,(x,0,1)),'\n')
```

(b)  
since the probability of survive given that you take the drug or given that you do not take the drug are the same, so they are the same.   
We cannot get the correct answer since there are people under drug condition who are not compliant.

```julia
integrate(3*x/4,(x,0,1)) - integrate((x+1)/4,(x,0,1))
```

(c)  
You should not take the drug. As $\frac{1}{24} = \frac{1}{24}$, you should not take the drug as it does not have effect on the probability of survival when you are not compliant.


## Problem 7

(a) Write a function which a distribution `D`, together with an $\alpha$ value and a positive integer $n$ and returns `true` or `false` according to whether the empirical CDF for a random sample of size $n$ obeys the bound in the DKW inequality. 

(b) Run the function many times with `α = 0.05` and check that it returns `true` around 95% of the time. 

```julia
using Roots
function DKW_check(D, α, n) 
    observation = sort(rand(D,n))
    ϵ = find_zero(ϵ -> 2exp(-2n*ϵ^2) - α, 0.1)
#     observation_low = sort(rand(D,n) .- ϵ)
#     observation_high = sort(rand(D,n) .+ ϵ)
# #     cdf_ = [cdf(D, obs) for obs in observation]
#     cdf_ = cdf(D,(1:n)/n)
#     for i in 1:n
#         if cdf_[i] < observation_low[i] || cdf_[i] > observation_high[i]
#             return false
#         end
#     end
#     return true
    for i in 1:n
        if abs(cdf(D,observation[i])-i/n) > ϵ
            return false
        end
    end
    return true
        
end
```

```julia
D = Uniform(0,1)
α = 0.05
n = 1000
mean(DKW_check(D, α, n) for _ in 1:10000)
```

## Problem 8

Consider a family of distributions of the form $\mathrm{Uniform}(\theta, \theta+1)$, where $\theta$ is a parameter. Given a sample $X_1, \ldots, X_n$, show that there isn't a unique maximum likelihood estimator for $\theta$


**Solution:**  
The $\mathbf{pdf}$ of $x_{i}$ is $\frac{1}{\theta + 1 - \theta} = 1$, thus the likelihood $\mathcal{L}_{x}{(\theta)} = f_{\theta}(x_{1})f_{\theta}(x_{2}) \dots f_{\theta}(x_{n}) = 1$, no matter what statistical functional $T$ we pick for $\theta$, it will not maximum the likelihood since the likelihood is just a constant. Thus, these isn't an unique maximum likelihood estimator for $\theta$.
